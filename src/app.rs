use crate::ai::{
    client::LLMClient, 
    commands::{CommandParser, CommandType},
    config::LLMConfig,
    streaming::{StreamHandler, StreamingChatResponse},
};
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

pub struct App {
    pub llm_client: Option<LLMClient>,
    pub llm_config: Option<LLMConfig>,
    pub project_context: Option<crate::ai::context::ProjectContext>,
    pub chat_input: String,
    pub chat_history: Vec<ChatMessage>,
    pub stream_handler: Arc<Mutex<Option<StreamHandler>>>,
    pub streaming_response: Arc<Mutex<StreamingChatResponse>>,
    pub is_streaming: bool,
}


impl App {
    pub fn new() -> Self {
        Self {
            llm_client: None,
            llm_config: None,
            project_context: None,
            chat_input: String::new(),
            chat_history: Vec::new(),
            stream_handler: Arc::new(Mutex::new(None)),
            streaming_response: Arc::new(Mutex::new(StreamingChatResponse::new())),
            is_streaming: false,
        }
    }

    pub fn init_ai_client(&mut self, api_key: String) {
        let config = crate::ai::client::LLMConfig {
            api_key,
            model: "gpt-3.5-turbo".to_string(),
            base_url: "https://api.openai.com/v1/chat/completions".to_string(),
            temperature: 0.7,
            max_tokens: 200,
        };
        
        self.llm_client = Some(LLMClient::new(config));
    }

    pub fn init_ai_client_with_config(&mut self, config: LLMConfig) {
        let llm_config = crate::ai::client::LLMConfig {
            api_key: config.api_key.clone(),
            model: config.model.clone(),
            base_url: config.base_url.clone(),
            temperature: config.temperature,
            max_tokens: config.max_tokens,
        };
        
        self.llm_config = Some(config);
        self.llm_client = Some(LLMClient::new(llm_config));
    }


    pub fn handle_chat_input(&mut self, c: char) {
        self.chat_input.push(c);
    }

    pub fn handle_chat_backspace(&mut self) {
        self.chat_input.pop();
    }

    pub fn handle_chat_submit(&mut self) {
        if !self.chat_input.trim().is_empty() {
            let input = self.chat_input.clone();
            
            // æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤
            if CommandParser::has_command(&input) {
                self.handle_command(&input);
            } else {
                // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
                self.chat_history.push(ChatMessage {
                    role: "user".to_string(),
                    content: input.clone(),
                });

                // å¤„ç†æåŠ
                let mentions = CommandParser::extract_mentions(&input);
                let mut response = String::new();
                
                for mention in mentions {
                    response.push_str(&self.process_mention(&mention));
                    response.push('\n');
                }

                // å¦‚æœæ²¡æœ‰æåŠï¼Œç”Ÿæˆ AI å“åº”
                if response.is_empty() {
                    response = format!("Echo: {}", input);
                }

                // æ·»åŠ  AI å“åº”åˆ°èŠå¤©å†å²
                self.chat_history.push(ChatMessage {
                    role: "assistant".to_string(),
                    content: response,
                });
            }

            // æ¸…ç©ºè¾“å…¥
            self.chat_input.clear();
        }
    }

    fn handle_command(&mut self, input: &str) {
        if let Some(cmd) = CommandParser::parse_command(input) {
            // æ·»åŠ ç”¨æˆ·å‘½ä»¤åˆ°èŠå¤©å†å²
            self.chat_history.push(ChatMessage {
                role: "user".to_string(),
                content: input.to_string(),
            });

            let response = match cmd.command_type {
                CommandType::Help => CommandParser::get_help(),
                CommandType::Clear => {
                    self.chat_history.clear();
                    "âœ“ èŠå¤©å†å²å·²æ¸…é™¤".to_string()
                }
                CommandType::History => {
                    if self.chat_history.is_empty() {
                        "èŠå¤©å†å²ä¸ºç©º".to_string()
                    } else {
                        let mut hist = String::from("ğŸ“œ èŠå¤©å†å²:\n");
                        for (i, msg) in self.chat_history.iter().enumerate() {
                            hist.push_str(&format!("{}. [{}]: {}\n", i + 1, msg.role, msg.content));
                        }
                        hist
                    }
                }
                CommandType::Model => {
                    if let Some(config) = &self.llm_config {
                        if cmd.args.is_empty() {
                            format!("ğŸ“Š å½“å‰æ¨¡å‹: {}", config.model)
                        } else {
                            format!("æ¨¡å‹è®¾ç½®ä¸º: {}", cmd.args.join(" "))
                        }
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Provider => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸ”Œ å½“å‰æä¾›å•†: {}", config.provider.to_string())
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Temperature => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸŒ¡ï¸ å½“å‰æ¸©åº¦: {}", config.temperature)
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::MaxTokens => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸ“ æœ€å¤§ä»¤ç‰Œæ•°: {}", config.max_tokens)
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Status => {
                    let mut status = String::from("ğŸ“ˆ åº”ç”¨çŠ¶æ€:\n");
                    if let Some(config) = &self.llm_config {
                        status.push_str(&format!("  æä¾›å•†: {}\n", config.provider.to_string()));
                        status.push_str(&format!("  æ¨¡å‹: {}\n", config.model));
                        status.push_str(&format!("  æ¸©åº¦: {}\n", config.temperature));
                        status.push_str(&format!("  æœ€å¤§ä»¤ç‰Œ: {}\n", config.max_tokens));
                    } else {
                        status.push_str("  LLM: æœªé…ç½®\n");
                    }
                    status.push_str(&format!("  èŠå¤©æ¶ˆæ¯æ•°: {}\n", self.chat_history.len()));
                    status
                }
                CommandType::Unknown => "âŒ æœªçŸ¥å‘½ä»¤ã€‚è¾“å…¥ /help è·å–å¸®åŠ©".to_string(),
            };

            // æ·»åŠ å‘½ä»¤å“åº”åˆ°èŠå¤©å†å²
            self.chat_history.push(ChatMessage {
                role: "system".to_string(),
                content: response,
            });
        }
    }

    fn process_mention(&self, mention: &crate::ai::commands::Mention) -> String {
        use crate::ai::commands::MentionType;
        
        match mention.mention_type {
            MentionType::Model => {
                if let Some(config) = &self.llm_config {
                    format!("ğŸ“Š [æ¨¡å‹: {}]", config.model)
                } else {
                    "[æ¨¡å‹: æœªé…ç½®]".to_string()
                }
            }
            MentionType::Provider => {
                if let Some(config) = &self.llm_config {
                    format!("ğŸ”Œ [æä¾›å•†: {}]", config.provider.to_string())
                } else {
                    "[æä¾›å•†: æœªé…ç½®]".to_string()
                }
            }
            MentionType::History => {
                format!("ğŸ“œ [èŠå¤©å†å²: {} æ¡æ¶ˆæ¯]", self.chat_history.len())
            }
            MentionType::File => {
                format!("ğŸ“„ [æ–‡ä»¶: {}]", mention.target)
            }
            MentionType::Unknown => {
                format!("[æœªçŸ¥æåŠ: {}]", mention.target)
            }
        }
    }

    /// å¯åŠ¨æµå¼èŠå¤©
    pub async fn start_streaming_chat(&mut self, prompt: &str) {
        if let Some(ref client) = self.llm_client {
            self.is_streaming = true;
            let handler = StreamHandler::new();
            *self.stream_handler.lock().await = Some(handler.clone());
            
            // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            self.chat_history.push(ChatMessage {
                role: "user".to_string(),
                content: prompt.to_string(),
            });

            let client = client.clone();
            let prompt = prompt.to_string();
            let handler = handler.clone();
            let streaming_response = Arc::clone(&self.streaming_response);

            // åœ¨åå°ä»»åŠ¡ä¸­å¤„ç†æµå¼å“åº”
            tokio::spawn(async move {
                let callback = |token: String| {
                    let _ = handler.send_token(token.clone());
                    true
                };

                match client.generate_completion_stream(&prompt, callback).await {
                    Ok(_) => {
                        let _ = handler.send_done();
                        let mut resp = streaming_response.lock().await;
                        resp.mark_complete();
                    }
                    Err(e) => {
                        let _ = handler.send_error(e.to_string());
                    }
                }
            });
        }
    }

    /// è·å–æµå¼å“åº”å†…å®¹
    pub async fn get_streaming_content(&self) -> String {
        self.streaming_response.lock().await.get_content().to_string()
    }

    /// å®Œæˆæµå¼å“åº”å¹¶æ·»åŠ åˆ°å†å²
    pub async fn finalize_streaming_response(&mut self) {
        let response = self.streaming_response.lock().await;
        if !response.get_content().is_empty() {
            self.chat_history.push(ChatMessage {
                role: "assistant".to_string(),
                content: response.get_content().to_string(),
            });
        }
        drop(response);
        
        // é‡ç½®æµå¼å“åº”
        self.streaming_response.lock().await.reset();
        self.is_streaming = false;
    }

}