use crate::ai::{
    client::LLMClient, 
    commands::{CommandParser, CommandType},
    config::LLMConfig,
    streaming::{StreamHandler, StreamingChatResponse},
};
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

pub struct App {
    pub llm_client: Option<LLMClient>,
    pub llm_config: Option<LLMConfig>,
    pub project_context: Option<crate::ai::context::ProjectContext>,
    pub chat_input: String,
    pub chat_history: Vec<ChatMessage>,
    pub stream_handler: Arc<Mutex<Option<StreamHandler>>>,
    pub streaming_response: Arc<Mutex<StreamingChatResponse>>,
    pub is_streaming: bool,
}


impl App {
    pub fn new() -> Self {
        Self {
            llm_client: None,
            llm_config: None,
            project_context: None,
            chat_input: String::new(),
            chat_history: Vec::new(),
            stream_handler: Arc::new(Mutex::new(None)),
            streaming_response: Arc::new(Mutex::new(StreamingChatResponse::new())),
            is_streaming: false,
        }
    }

    pub fn init_ai_client(&mut self, api_key: String) {
        let config = crate::ai::client::LLMConfig {
            api_key,
            model: "gpt-3.5-turbo".to_string(),
            base_url: "https://api.openai.com/v1/chat/completions".to_string(),
            temperature: 0.7,
            max_tokens: 200,
        };
        
        self.llm_client = Some(LLMClient::new(config));
    }

    pub fn init_ai_client_with_config(&mut self, config: LLMConfig) {
        let llm_config = crate::ai::client::LLMConfig {
            api_key: config.api_key.clone(),
            model: config.model.clone(),
            base_url: config.base_url.clone(),
            temperature: config.temperature,
            max_tokens: config.max_tokens,
        };
        
        self.llm_config = Some(config);
        self.llm_client = Some(LLMClient::new(llm_config));
    }

    /// æ›´æ–° LLM å®¢æˆ·ç«¯é…ç½®
    fn update_llm_client(&mut self) {
        if let Some(config) = &self.llm_config {
            let llm_config = crate::ai::client::LLMConfig {
                api_key: config.api_key.clone(),
                model: config.model.clone(),
                base_url: config.base_url.clone(),
                temperature: config.temperature,
                max_tokens: config.max_tokens,
            };
            
            self.llm_client = Some(LLMClient::new(llm_config));
        }
    }


    pub fn handle_chat_input(&mut self, c: char) {
        self.chat_input.push(c);
    }

    pub fn handle_chat_backspace(&mut self) {
        self.chat_input.pop();
    }

    pub fn handle_chat_submit(&mut self) {
        if !self.chat_input.trim().is_empty() {
            let input = self.chat_input.clone();
            
            // æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤
            if CommandParser::has_command(&input) {
                self.handle_command(&input);
            } else {
                // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
                self.chat_history.push(ChatMessage {
                    role: "user".to_string(),
                    content: input.clone(),
                });

                // å¤„ç†æåŠ
                let mentions = CommandParser::extract_mentions(&input);
                let mut response = String::new();
                
                for mention in mentions {
                    response.push_str(&self.process_mention(&mention));
                    response.push('\n');
                }

                // å¦‚æœæ²¡æœ‰æåŠï¼Œç”Ÿæˆ AI å“åº”
                if response.is_empty() {
                    response = format!("Echo: {}", input);
                }

                // æ·»åŠ  AI å“åº”åˆ°èŠå¤©å†å²
                self.chat_history.push(ChatMessage {
                    role: "assistant".to_string(),
                    content: response,
                });
            }

            // æ¸…ç©ºè¾“å…¥
            self.chat_input.clear();
        }
    }

    fn handle_command(&mut self, input: &str) {
        if let Some(cmd) = CommandParser::parse_command(input) {
            // æ·»åŠ ç”¨æˆ·å‘½ä»¤åˆ°èŠå¤©å†å²
            self.chat_history.push(ChatMessage {
                role: "user".to_string(),
                content: input.to_string(),
            });

            let response = match cmd.command_type {
                CommandType::Help => CommandParser::get_help(),
                CommandType::Clear => {
                    self.chat_history.clear();
                    "âœ“ èŠå¤©å†å²å·²æ¸…é™¤".to_string()
                }
                CommandType::History => {
                    if self.chat_history.is_empty() {
                        "èŠå¤©å†å²ä¸ºç©º".to_string()
                    } else {
                        let mut hist = String::from("ğŸ“œ èŠå¤©å†å²:\n");
                        for (i, msg) in self.chat_history.iter().enumerate() {
                            hist.push_str(&format!("{}. [{}]: {}\n", i + 1, msg.role, msg.content));
                        }
                        hist
                    }
                }
                CommandType::Model => {
                    if let Some(config) = &self.llm_config {
                        if cmd.args.is_empty() {
                            format!("ğŸ“Š å½“å‰æ¨¡å‹: {}", config.model)
                        } else {
                            format!("æ¨¡å‹è®¾ç½®ä¸º: {}", cmd.args.join(" "))
                        }
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Provider => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸ”Œ å½“å‰æä¾›å•†: {}", config.provider.to_string())
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Temperature => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸŒ¡ï¸ å½“å‰æ¸©åº¦: {}", config.temperature)
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::MaxTokens => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸ“ æœ€å¤§ä»¤ç‰Œæ•°: {}", config.max_tokens)
                    } else {
                        "æœªé…ç½® LLM".to_string()
                    }
                }
                CommandType::Status => {
                    if let Some(config) = &self.llm_config {
                        format!("ğŸ“ˆ {}", config.get_status_info())
                    } else {
                        "âŒ LLM æœªé…ç½®".to_string()
                    }
                }
                CommandType::ListProviders => {
                    let mut response = String::from("ğŸ”Œ å¯ç”¨çš„ AI æä¾›å•†:\n\n");
                    for (provider, description) in LLMConfig::list_providers() {
                        response.push_str(&format!("â€¢ {}: {}\n", provider.to_string(), description));
                    }
                    response
                }
                CommandType::SetProvider => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æŒ‡å®šæä¾›å•†åç§°ã€‚ä¾‹å¦‚: /set-provider openai".to_string()
                    } else {
                        let provider_name = &cmd.args[0];
                        let provider = crate::ai::config::LLMProvider::from_string(provider_name);
                        
                        if let Some(config) = &mut self.llm_config {
                            config.set_provider(provider.clone());
                            self.update_llm_client();
                            format!("âœ“ æä¾›å•†å·²åˆ‡æ¢åˆ°: {}", provider.to_string())
                        } else {
                            "âŒ è¯·å…ˆé…ç½® LLM".to_string()
                        }
                    }
                }
                CommandType::SetApiKey => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾› API å¯†é’¥ã€‚ä¾‹å¦‚: /set-api-key your-key-here".to_string()
                    } else {
                        let api_key = cmd.args.join(" ");
                        if let Some(config) = &mut self.llm_config {
                            config.api_key = api_key;
                            self.update_llm_client();
                            "âœ“ API å¯†é’¥å·²æ›´æ–°".to_string()
                        } else {
                            "âŒ è¯·å…ˆé…ç½® LLM".to_string()
                        }
                    }
                }
                CommandType::SetModel => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æŒ‡å®šæ¨¡å‹åç§°ã€‚ä¾‹å¦‚: /set-model gpt-4".to_string()
                    } else {
                        let model = cmd.args.join(" ");
                        if let Some(config) = &mut self.llm_config {
                            config.model = model.clone();
                            self.update_llm_client();
                            format!("âœ“ æ¨¡å‹å·²è®¾ç½®ä¸º: {}", model)
                        } else {
                            "âŒ è¯·å…ˆé…ç½® LLM".to_string()
                        }
                    }
                }
                CommandType::SetBaseUrl => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾›åŸºç¡€ URLã€‚ä¾‹å¦‚: /set-base-url https://api.example.com".to_string()
                    } else {
                        let base_url = cmd.args.join(" ");
                        if let Some(config) = &mut self.llm_config {
                            config.base_url = base_url.clone();
                            self.update_llm_client();
                            format!("âœ“ åŸºç¡€ URL å·²è®¾ç½®ä¸º: {}", base_url)
                        } else {
                            "âŒ è¯·å…ˆé…ç½® LLM".to_string()
                        }
                    }
                }
                CommandType::ConfigOpenAI => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾› API å¯†é’¥ã€‚ä¾‹å¦‚: /openai sk-your-key gpt-4".to_string()
                    } else {
                        let api_key = cmd.args[0].clone();
                        let model = cmd.args.get(1).cloned();
                        
                        if let Some(config) = &mut self.llm_config {
                            config.quick_config_openai(api_key, model.clone());
                        } else {
                            let mut new_config = LLMConfig::default_openai(api_key);
                            if let Some(m) = model.clone() {
                                new_config.model = m;
                            }
                            self.llm_config = Some(new_config);
                        }
                        self.update_llm_client();
                        format!("âœ“ OpenAI é…ç½®å®Œæˆ - æ¨¡å‹: {}", 
                               model.unwrap_or_else(|| "gpt-3.5-turbo".to_string()))
                    }
                }
                CommandType::ConfigClaude => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾› API å¯†é’¥ã€‚ä¾‹å¦‚: /claude your-key claude-3-opus".to_string()
                    } else {
                        let api_key = cmd.args[0].clone();
                        let model = cmd.args.get(1).cloned();
                        
                        if let Some(config) = &mut self.llm_config {
                            config.quick_config_claude(api_key, model.clone());
                        } else {
                            let mut new_config = LLMConfig::default_openai(api_key); // ä¸´æ—¶ä½¿ç”¨ï¼Œä¼šè¢«è¦†ç›–
                            new_config.quick_config_claude(new_config.api_key.clone(), model.clone());
                            self.llm_config = Some(new_config);
                        }
                        self.update_llm_client();
                        format!("âœ“ Claude é…ç½®å®Œæˆ - æ¨¡å‹: {}", 
                               model.unwrap_or_else(|| "claude-3-sonnet".to_string()))
                    }
                }
                CommandType::ConfigGemini => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾› API å¯†é’¥ã€‚ä¾‹å¦‚: /gemini your-key gemini-pro".to_string()
                    } else {
                        let api_key = cmd.args[0].clone();
                        let model = cmd.args.get(1).cloned();
                        
                        if let Some(config) = &mut self.llm_config {
                            config.quick_config_gemini(api_key, model.clone());
                        } else {
                            let mut new_config = LLMConfig::default_gemini(api_key);
                            if let Some(m) = model.clone() {
                                new_config.model = m;
                            }
                            self.llm_config = Some(new_config);
                        }
                        self.update_llm_client();
                        format!("âœ“ Gemini é…ç½®å®Œæˆ - æ¨¡å‹: {}", 
                               model.unwrap_or_else(|| "gemini-1.5-flash".to_string()))
                    }
                }
                CommandType::ConfigOllama => {
                    let model = cmd.args.get(0).cloned();
                    let base_url = cmd.args.get(1).cloned();
                    
                    if let Some(config) = &mut self.llm_config {
                        config.quick_config_ollama(model.clone(), base_url.clone());
                    } else {
                        let mut new_config = LLMConfig::default_ollama();
                        if let Some(m) = model.clone() {
                            new_config.model = m;
                        }
                        if let Some(url) = base_url.clone() {
                            new_config.base_url = url;
                        }
                        self.llm_config = Some(new_config);
                    }
                    self.update_llm_client();
                    format!("âœ“ Ollama é…ç½®å®Œæˆ - æ¨¡å‹: {}", 
                           model.unwrap_or_else(|| "mistral".to_string()))
                }
                CommandType::ConfigLocal => {
                    if cmd.args.is_empty() {
                        "âŒ è¯·æä¾›æœåŠ¡å™¨ URLã€‚ä¾‹å¦‚: /local http://localhost:1234/v1/chat/completions".to_string()
                    } else {
                        let base_url = cmd.args[0].clone();
                        let model = cmd.args.get(1).cloned();
                        
                        if let Some(config) = &mut self.llm_config {
                            config.quick_config_local(base_url.clone(), model.clone());
                        } else {
                            let new_config = LLMConfig::default_local_server(base_url.clone());
                            self.llm_config = Some(new_config);
                        }
                        self.update_llm_client();
                        format!("âœ“ æœ¬åœ°æœåŠ¡å™¨é…ç½®å®Œæˆ - URL: {}", base_url)
                    }
                }
                CommandType::SaveConfig => {
                    if let Some(config) = &self.llm_config {
                        match config.save_to_env() {
                            Ok(_) => "âœ“ é…ç½®å·²ä¿å­˜åˆ° .env æ–‡ä»¶".to_string(),
                            Err(e) => format!("âŒ ä¿å­˜é…ç½®å¤±è´¥: {}", e),
                        }
                    } else {
                        "âŒ æ²¡æœ‰é…ç½®å¯ä¿å­˜".to_string()
                    }
                }
                CommandType::LoadConfig => {
                    match LLMConfig::from_env() {
                        Ok(config) => {
                            self.llm_config = Some(config);
                            self.update_llm_client();
                            "âœ“ é…ç½®å·²ä» .env æ–‡ä»¶é‡æ–°åŠ è½½".to_string()
                        }
                        Err(e) => format!("âŒ åŠ è½½é…ç½®å¤±è´¥: {}", e),
                    }
                }
                CommandType::Unknown => "âŒ æœªçŸ¥å‘½ä»¤ã€‚è¾“å…¥ /help è·å–å¸®åŠ©".to_string(),
            };

            // æ·»åŠ å‘½ä»¤å“åº”åˆ°èŠå¤©å†å²
            self.chat_history.push(ChatMessage {
                role: "system".to_string(),
                content: response,
            });
        }
    }

    fn process_mention(&self, mention: &crate::ai::commands::Mention) -> String {
        use crate::ai::commands::MentionType;
        
        match mention.mention_type {
            MentionType::Model => {
                if let Some(config) = &self.llm_config {
                    format!("ğŸ“Š [æ¨¡å‹: {}]", config.model)
                } else {
                    "[æ¨¡å‹: æœªé…ç½®]".to_string()
                }
            }
            MentionType::Provider => {
                if let Some(config) = &self.llm_config {
                    format!("ğŸ”Œ [æä¾›å•†: {}]", config.provider.to_string())
                } else {
                    "[æä¾›å•†: æœªé…ç½®]".to_string()
                }
            }
            MentionType::History => {
                format!("ğŸ“œ [èŠå¤©å†å²: {} æ¡æ¶ˆæ¯]", self.chat_history.len())
            }
            MentionType::File => {
                format!("ğŸ“„ [æ–‡ä»¶: {}]", mention.target)
            }
            MentionType::Unknown => {
                format!("[æœªçŸ¥æåŠ: {}]", mention.target)
            }
        }
    }

    /// å¯åŠ¨æµå¼èŠå¤©
    pub async fn start_streaming_chat(&mut self, prompt: &str) {
        if let Some(ref client) = self.llm_client {
            self.is_streaming = true;
            let handler = StreamHandler::new();
            *self.stream_handler.lock().await = Some(handler.clone());
            
            // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            self.chat_history.push(ChatMessage {
                role: "user".to_string(),
                content: prompt.to_string(),
            });

            let client = client.clone();
            let prompt = prompt.to_string();
            let handler = handler.clone();
            let streaming_response = Arc::clone(&self.streaming_response);

            // åœ¨åå°ä»»åŠ¡ä¸­å¤„ç†æµå¼å“åº”
            tokio::spawn(async move {
                let callback = |token: String| {
                    let _ = handler.send_token(token.clone());
                    true
                };

                match client.generate_completion_stream(&prompt, callback).await {
                    Ok(_) => {
                        let _ = handler.send_done();
                        let mut resp = streaming_response.lock().await;
                        resp.mark_complete();
                    }
                    Err(e) => {
                        let _ = handler.send_error(e.to_string());
                    }
                }
            });
        }
    }

    /// è·å–æµå¼å“åº”å†…å®¹
    pub async fn get_streaming_content(&self) -> String {
        self.streaming_response.lock().await.get_content().to_string()
    }

    /// å®Œæˆæµå¼å“åº”å¹¶æ·»åŠ åˆ°å†å²
    pub async fn finalize_streaming_response(&mut self) {
        let response = self.streaming_response.lock().await;
        if !response.get_content().is_empty() {
            self.chat_history.push(ChatMessage {
                role: "assistant".to_string(),
                content: response.get_content().to_string(),
            });
        }
        drop(response);
        
        // é‡ç½®æµå¼å“åº”
        self.streaming_response.lock().await.reset();
        self.is_streaming = false;
    }

}